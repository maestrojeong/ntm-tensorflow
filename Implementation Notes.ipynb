{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.linalg as ln\n",
    "from model.ntm_ops import *\n",
    "from model.memory import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Neural Turing Machines combined the ability of Turing Machine and Neural Networks to infer simple algorithms. The controller (it's usually a LSTM) can be viewed as CPU and the external memory can be seen as RAM. \n",
    "\n",
    "A NTM has four components: Controller, read heads, write heads, and an external memory. \n",
    "\n",
    "High level overview:\n",
    "1. Addressing: Addressing mechanism is used to produce the weightings of each head. There are two types of adrressing, content based and location based. At every time step, the controller outputs five elements to produce weightings of each head: key vector, key strength, interpolation gate, shift weighting, and a scalar that used to sharpen the weightings. \n",
    "2. Read: each read head has a weighting vector tells how much degree of information we read from on each memory location\n",
    "3. Write: each write head has a weighting vector, an erase vector and an add vector. This is inspired by LSTM's forget gate and input gate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Memory matrix\n",
    "Define two hyper parameters for the memory matrix: $N \\times M$, where $N$ is the number of memory locations, $M$ is the vector size at each memory location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Controller dimension\n",
    "Define the LSTM hidden state dimension h and stacked hidden layer number a. This is the same as tradition LSTM with the hidden state and cell state.\n",
    "\n",
    "Define the output and input dimension, in NTM, it usually is how many bits per sequence. e.g. If one of the input sequence is [0, 1, 0, 1, 0, 1], then it should be 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 The range of allowed location shift\n",
    "Define the range of the allowed location shift in location based addressing (Convolutional shift), s. e.g. if s = 3, then allowed location shift will be [-1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Memory Testing\n",
    "At every time step the controller outputs weighting of each head and hidden states(including cell states in original LSTM).. The weighting is determined by addressing mechanism:\n",
    "1. Content Addressing\n",
    "2. Interpolation\n",
    "3. Convolutional Shift\n",
    "4. Sharpening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define memory and give attributes values\n",
    "B, N, M = 1, 3, 2\n",
    "memory = Memory(batch_size=B, mem_dim=M, mem_size=N)\n",
    "memory.memory = tf.constant(np.array([[\n",
    "            [6, 8],\n",
    "            [1, 2],\n",
    "            [3, 3],\n",
    "            \n",
    "        ]]), dtype=tf.float32)\n",
    "memory.read_weighting = tf.constant(np.array([\n",
    "            [1, 0, 0]\n",
    "        ]), dtype=tf.float32)\n",
    "memory.write_weighting = tf.constant(np.array([\n",
    "            [0, 1, 0]\n",
    "        ]), dtype=tf.float32)\n",
    "memory.read_vector = tf.constant(np.array([\n",
    "            [1.0, 1.0]\n",
    "        ]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define emitted vectors, these should be generated by the controller.\n",
    "key_vector = tf.constant(np.array([[3, 4]]), dtype=tf.float32)\n",
    "key_strength = tf.constant(np.array([[1]]), dtype=tf.float32)\n",
    "interplotation = tf.constant(np.array([[0.9]]), dtype=tf.float32)\n",
    "shifting = tf.constant([[0, 0, 1]], dtype=tf.float32)\n",
    "sharpening = tf.constant([[2]], dtype=tf.float32)\n",
    "add_vector = tf.constant([\n",
    "        [0.5, 0.5]\n",
    "    ])\n",
    "erase_vector = tf.constant([\n",
    "        [0.1, 0.5]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous memory\n",
      "[[[ 6.  8.]\n",
      "  [ 1.  2.]\n",
      "  [ 3.  3.]]]\n",
      "Current memory\n",
      "[[[ 6.          8.        ]\n",
      "  [ 1.39999998  1.5       ]\n",
      "  [ 3.          3.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# memory write test\n",
    "print \"Previous memory\\n\", memory.memory.eval()\n",
    "matrix = memory.write(memory.write_weighting, memory.memory, erase_vector, add_vector)\n",
    "memory.memory = matrix\n",
    "print \"Current memory\\n\", memory.memory.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current write weighting [[ 0.  1.  0.]]\n",
      "next write weighting [[ 0.32519153  0.32578066  0.34902781]]\n"
     ]
    }
   ],
   "source": [
    "# update write weighting test\n",
    "print \"current write weighting\", memory.write_weighting.eval()\n",
    "memory.write_weighting = memory.update_weighting(key_vector, key_strength, interplotation, shifting, sharpening,\n",
    "                                                 memory.write_weighting, memory.memory)\n",
    "print \"next write weighting\", memory.write_weighting.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current read vector [[ 1.  1.]]\n",
      "next read vector [[ 6.  8.]]\n"
     ]
    }
   ],
   "source": [
    "# memory read test\n",
    "print \"current read vector\", memory.read_vector.eval()\n",
    "memory.read_vector = memory.read(memory.read_weighting, memory.memory)\n",
    "print \"next read vector\", memory.read_vector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current read weighting [[ 1.  0.  0.]]\n",
      "next read weighting [[ 0.32514414  0.34946215  0.32539377]]\n"
     ]
    }
   ],
   "source": [
    "# update read weighting test\n",
    "print \"current read weighting\", memory.read_weighting.eval()\n",
    "memory.read_weighting = memory.update_weighting(key_vector, key_strength, interplotation, shifting, sharpening,\n",
    "                                                 memory.read_weighting, memory.memory)\n",
    "print \"next read weighting\", memory.read_weighting.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "_LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n",
    "\n",
    "class LSTMStateTuple(_LSTMStateTuple):\n",
    "  \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n",
    "  Stores two elements: `(c, h)`, in that order.\n",
    "  Only used when `state_is_tuple=True`.\n",
    "  \"\"\"\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (c, h) = self\n",
    "    if not c.dtype == h.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(c.dtype), str(h.dtype)))\n",
    "    return c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = LSTMStateTuple(tf.constant(np.array([0, 1, 2])), tf.constant(np.array([0, 1, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}